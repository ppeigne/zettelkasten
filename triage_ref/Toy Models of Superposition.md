Source: 
Consider: [[]]
Tags: 
______________

# Key takeaways

- Superposition is real.
- Neurons can be polysemantics. or monosemantics.
- Some computation is done in superpositions. Superposition is not only useful for encoding features, but allows 'superposed' computation.
- Phase change governs superposition.
- Features are organized into geometric structures such as digon, triangles, pentagons or tetrahedrons.

Definition and motivation
Features of the input space are represented as directions in the activation space.
- Decomposability: neural net representation are independently understandable features
- Linearity: features are represented as directions in the neurons vector space.

Priviledge basis: in a privilidge basis features are aligned with directions of the vector space.

Definition Features:
Features are neurons in a sufficiently large model.
Properties of the input which a sufficiently large neural net will reliably dedicate a neuron to representig. #FIXME (Double check the def)

Features as directions:
Features are directions in the activation space. There is a linear mapping between the features and the activation space. 

Linear representation:
- Natural outputs of obvious algorithm a layer might implement. The more a neuron match a template the input linearly the better it reacts. #FIXME Why?
- Linear representation make features linearly accessible. #FIXME Why?
- Statistical efficacy: non-local generalization #FIXME ?



