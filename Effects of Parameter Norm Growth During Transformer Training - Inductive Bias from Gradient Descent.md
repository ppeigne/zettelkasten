Source: 
Consider: [[]]
Tags: 
______________

# Claim
Successful learning of transformer models is due to inductive bias in the training routine. 
The training routine drives the growth of the layer norm. The growth of the later norm results in (or is correlated with) the network approximating a discretized network with saturated activation functions.  


Norm growth of layers induces saturation.
Saturated large language models' heads approximate two kinds of behavior/operations:
- argmax
- mean

# Key takeaways

