Previous: [[202207131013 - Transformer attention as Q,K, V operation]]
Source: [[Transformers from scratch]]
Consider: [[]]
Tags: #attention #neural_turing_machine #transformer 
______________

How did the attention worked in writing/reading architectures such as Neural Turing Machines? Did the 'attention' operations use three matrices $Q,V,K$ as well?