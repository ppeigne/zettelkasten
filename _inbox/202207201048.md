Previous: [[]]
Source: [[]]
Consider: [[]]
Tags: 
______________

Having 
- $n_\text{heads} = 12$
- $d_\text{head} = 64$
- $d_\text{model} = n_\text{head} * d_\text{head} = 768$

Why are the matrices $W_Q^h$, $W_K^h$ and $W_V^h$ for only one attention head $h$ of dimensions $[d_\text{head} \times d_\text{model}]$?

Given the fact that they are limited to one head, shouldn't they have a dimension of $[d_\text{head} \times d_\text{head}]$ instead?