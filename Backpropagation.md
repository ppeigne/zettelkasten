# Back propagation 

Back propagation is the central element of the [[training]] of [[neural network]] and other [[machine learning]] [[algorithm]]. 
It is the updating step of an algorithm following the forward [[propagation]] (or [[prediction]]). It usually relies heavily on [[Gradient based optimization]] algorithms.

The forward propagation generates output of the model given specific inputs. Then the back propagation update the [[weights]] and [[biases]] of the model according to its error on 

#deeplearning
#backpropagation
