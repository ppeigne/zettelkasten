# Back propagation 

Back propagation is a central element of the [[training]] of [[neural network | neural networks]] and other [[machine learning]] [[algorithm | algorithms]]. 
The backward propagation calculates the derivatives for each [[layer]] using the [[chain rule]]. 
[[Gradient based optimization]] algorithms relies heavily on it to perform [[gradient descent]].  



**to refacto**
enabling them to update the [[weights]] and [[biases]] of models according to their errors, using various forms of [[gradient descent]].

It is the updating step of an algorithm following the forward [[propagation]] (or [[prediction]]). It usually relies heavily on [[Gradient based optimization]] algorithms.

The forward propagation generates output of the model given specific inputs. Then the back propagation update the [[weights]] and [[biases]] of the model according to its error on 

#deeplearning
#backpropagation
