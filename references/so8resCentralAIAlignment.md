---
cssclass: research-note
type: "journalArticle"
author: "So8res, "
title: "A central AI alignment problem: capabilities generalization, and the sharp left turn"
citekey: so8resCentralAIAlignment
---
So8res. “A Central AI Alignment Problem: Capabilities Generalization, and the Sharp Left Turn.” Accessed November 16, 2022. [https://www.alignmentforum.org/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization](https://www.alignmentforum.org/posts/GNhMPAWcfBCASy8e6/a-central-ai-alignment-problem-capabilities-generalization).
[online](http://zotero.org/users/10595302/items/QZ9VFFVT) [local](zotero://select/library/items/QZ9VFFVT) [pdf](file:///home/research/snap/zotero-snap/common/Zotero/storage/UNXSYIU5/A%20central%20AI%20alignment%20problem%20capabilities%20generalization,%20and%20the%20sharp%20left%20turn%20-%20AI%20Alignment%20Forum.pdf)
 


# Key takeaways

- **Capabilities generalize better than alignment.**
- The hard alignment problem is therefore to find a way to either:
	- keep the system aligned while taking a sharp left turn / sliding into the capabilities well
	- realign the system afterward
- Nobody is working on this hard problem

### Index

start-date:: 
end-date::
page-no:: 2

### Connections

comment:: 

### Note
%% begin annotations %%
### Imported on 2022-11-16 1:46 pm

On my model� one of the most central technical challenges of alignment�and one that every viable alignment plan will probably need to grapple with�is the issue that capabilities generalize better than alignment� [(p. 2)](zotero://open-pdf/library/items/UNXSYIU5?page=2&annotation=F3TMYIP3)

>[!quote|#a28ae5] Undefined - Purple
>
there will at some point be some sort of �sharp left turn�� as systems start to work really well in domains really far beyond the environments of their training�domains that allow for signi�cant reshaping of the world� in the way that humans reshape the world and chimps don�t� And that�s where �according to me� things start to get crazy� In particular� I think that once AI capabilities start to generalize in this particular way� it’s predictably the case that the alignment of the system will fail to generalize with it� [(p. 3)](zotero://open-pdf/library/items/UNXSYIU5?page=3&annotation=KNRY35LI)

>[!quote|#a28ae5] Undefined - Purple
>
Why does alignment fail while capabilities generalize� at least by default and in predictable practice? In large part� because good capabilities form something like an attractor well� [(p. 4)](zotero://open-pdf/library/items/UNXSYIU5?page=4&annotation=M8YYSN8S)

>[!quote|#a28ae5] Undefined - Purple
>
There�s no analogous alignment well to slide into� [(p. 5)](zotero://open-pdf/library/items/UNXSYIU5?page=5&annotation=BYG445QJ)

>[!quote|#a28ae5] Undefined - Purple
>
On the contrary� sliding down the capabilities well is liable to break a bunch of your existing alignment properties� [(p. 5)](zotero://open-pdf/library/items/UNXSYIU5?page=5&annotation=SWZJVZBA)

my position is that capabilities generalize further than alignment �once capabilities start to generalize real well �which is a thing I predict will happen��� And this� by default� ruins your ability to direct the AGI �that has slipped down the capabilities well�� and breaks whatever constraints you were hoping would keep it corrigible� And addressing the problem looks like �nding some way to either keep your system aligned through that sharp left turn� or render it aligned afterwards [(p. 5)](zotero://open-pdf/library/items/UNXSYIU5?page=5&annotation=N8HWDD9U) %% end annotations %%

%% Import Date: 2022-11-16T13:46:58.958+01:00 %%
