Source: 
Consider: [[]]
Tags: 
______________

# Key takeaways

Solving the alignment could done by either:
- reducing the gap between the capabilities of aligned and unaligned AGIs (i.e. theoretical alignment)
- reducing the theory-practive gap (i.e. applied research)

---
cssclass: research-note
type: "journalArticle"
author: "Buck, "
title: "The theory-practice gap"
citekey: buckTheorypracticeGap
---
Buck. “The Theory-Practice Gap.” Accessed November 16, 2022. [https://www.alignmentforum.org/posts/xRyLxfytmLFZ6qz5s/the-theory-practice-gap](https://www.alignmentforum.org/posts/xRyLxfytmLFZ6qz5s/the-theory-practice-gap).
[online](http://zotero.org/users/10595302/items/H49SQB4M) [local](zotero://select/library/items/H49SQB4M) [pdf](file:///home/research/snap/zotero-snap/common/Zotero/storage/348RCKIZ/Buck_The%20theory-practice%20gap.pdf)
 

 


### Index

start-date:: 
end-date::
page-no:: 3

### Connections

comment:: 

### Note
%% begin annotations %%
### Imported on 2022-11-16 11:56 am

I�ve drawn this bar lower because I think that if your system is trying to imitate cognition that can be broken down into human understandable parts� it is systematically not going to be able to pursue certain powerful strategies that the end�to�end trained systems will be able to� I think that there are probably a bunch of concepts that humans can’t understand quickly� or maybe can’t understand at all� And if your systems are restricted to never use these concepts� I think your systems are probably just going to be a bunch weaker� [(p. 3)](zotero://open-pdf/library/items/348RCKIZ?page=3&annotation=FJD2J587)
%%being human-understandable put a huge constraint on the system and force it to not explore powerful concepts and heuristics which do not make sense fore humans%%

I claim that current alignment proposals don’t seem like they can control systems as powerful as the systems you’d get from an unaligned training strategy� [(p. 3)](zotero://open-pdf/library/items/348RCKIZ?page=3&annotation=IC7BXPVN)

empirical generalization is an example of relying on empirical facts about neural nets that are not true of arbitrary general black box function approximators� [(p. 3)](zotero://open-pdf/library/items/348RCKIZ?page=3&annotation=UUDTE6D9)

the maximum capability of an aligned AI that we can build is lower than the maximum achievable theoretically from the techniques we know about and empirical generalization [(p. 5)](zotero://open-pdf/library/items/348RCKIZ?page=5&annotation=V8UGAJLT)

the proposed alignment schemes still aren’t clearly good enough [(p. 6)](zotero://open-pdf/library/items/348RCKIZ?page=6&annotation=LANF48XC)

I think that these arguments distract from the claim that the theoretical alignment problem might be unsolved even if these problems are absent� [(p. 6)](zotero://open-pdf/library/items/348RCKIZ?page=6&annotation=LS68A8VB)

• we�re going to make substantial theoretical improvements • factored cognition is true • we�re going to have really good empirical generalization [(p. 6)](zotero://open-pdf/library/items/348RCKIZ?page=6&annotation=4SAY2M4I)

>[!quote|#5fb236] Reference
>
try to improve the best alignment techniques� I think this is what a lot of AI alignment theoretical work is [(p. 7)](zotero://open-pdf/library/items/348RCKIZ?page=7&annotation=THKLQXBM)

>[!quote|#5fb236] Reference
>
reduce the theory�practice gap� I think this is a pretty good description of what I think applied alignment research is usually trying to do� [(p. 7)](zotero://open-pdf/library/items/348RCKIZ?page=7&annotation=HUWC9CK5)

>[!quote|#5fb236] Reference
>
![[imports/buckTheorypracticeGap/TD4BPWLE.png]] 

>[!quote|#5fb236] Reference
>
You can try to improve our understanding of the relative height of all these bars� [(p. 7)](zotero://open-pdf/library/items/348RCKIZ?page=7&annotation=LIJ2GKSM) %% end annotations %%

%% Import Date: 2022-11-16T11:56:17.856+01:00 %%
