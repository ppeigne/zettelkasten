Previous: [[]]
Source: [[]]
Consider: [[]]
Tags: 
______________

## Why is it surprising, from the perspective of classic machine learning, that neural nets work so well? 

1. Overparametrized network can generalize:
Overparametrized neural nets (i.e. models with more parameters than training examples) can have very good generalization error (i.e. performance on the validation set).
Based on the classic machine learning perspective such overparametrized models should overfit their training data instead of learning to generalize, and therefore perform poorly on the validation set. For instance ResNet18 (11M parameters) and was trained on the ImageNet dataset (1.2M samples) and performs very well (3.57% test error). These impressive results are a theoretical conundrum. *Today, this conundrum could be obfuscated by the fact that Large Language Models are not overparametrized (e.g. GTP3 has 175B parameters and was trained on a 499B tokens dataset) but this state of affairs does not change the fact that we still do not understand why overparametrized networks generalize so well.*


A response to that phenomenon was to suggest that models' capacities are limited by some phenomenons and therefore constrain their capacity to memorize (e.g. internal capacity limitation or implicit regularization processes). However, experiments by Zhang et al (2017) training models on random noise have shown that neural nets are able to learn their entire dataset. These findings challenge the idea of internal constraint limiting the memorization and were followed by other intriguing findings of Arpit et al. (2017): generalization arises first and then memorization comes into play. 

2. For classification task generalization appears first, memorization then

Strange compared to grokking




Nns are overparametrized. 
Being overparametrized they should overfit their training data. 
However they are still able to reach a very low generalization error (test error). 

To explain this, two main line of inquiries have been investigated:
1. (Implicit/Explicit) Regularization. These models are subject to enough regularization to not be able to to overfit.
2. Capacities bounds. These models capacities are bounded in a way which force them to not learn the entire dataset (#FIXME: double check) 
	Bounded by: gradient based learning and early stopping

However, Zhang et al (2017) have shown that NNs are able to learn the entire dataset and that regularization does not impede overfitting (just slow it down). 


#######
- Priors (architecture: CNN, MLP) controls the speed of learning (Montavon et al. 2011)
- Number of parameters does not control the effective capacity of a DNN -> reason for generalization is unknown (Neyshabur et al. 2014)
- Impact of representational capacity changes with the data (degree of noise) (Arpit et al. 2017) + generalization first memorization after



## And why do they work so well?

Generalization comes from flat minima (which are much much bigger in volume than shaper ones) and memorization requires more complex/sharper decision boundaries (which probably implies sharper minima as well)