Problem statement
# The steering problem
"Using black-box access to human-level cognitive abilities, can we write a program that is as *useful* as *well-motivated* human with those abilities?"

-> **useful**: "another task at which a machine can reach human performance" but a different kind of task than "plan well", "reason well" or "predict well"
Difficult task because it relies on delegation, i.e. robust and efficient communication of desired behaviors.

"A program **P** is more useful than Hugh for X if, for every project using **H** to accomplish X, we can efficiently transform it into a new project which uses **P** to accomplish X." -> replacement by a program without any side-effects

-> **well motivated**: aligned with its designer #FIXME 
"For any human Hugh, if we run our program using Hugh-level black boxes, it should be as useful as Hugh"


Why now ? 
1. Maybe it's hard (actually it seems to be very hard)
2. It may help us understand AI
3. It should be developed alongside AI
4. Nine women can't make a baby in one month
5. AI progress may be surprising
6. The field is sparse

# Clarifying the "AI Alignment"
Ai $A$ is aligned with an operator $H$ if $A$ is **trying** to do **what $H$ wants** it to do.

The problem: getting the AI trying to do the right thing (the thing which seems right to the AI given its priors) not figuring out what is right. It should try to figure out but can fail.
-> Concerns the motives not the knowledge. Being more knowledgeable does not make you more aligned. Being more inclined to do what H wants make you more aligned.

De dicto definition is better than de re because it does not encompass sub problems which are : 
- probably totally different (solve differently) than the "trying to do what H wants" problem
- less urgent

Trying to do what H wants implies to try to understand H's preferences.

-> Imprecise about how to evaluates motives for AIs
-> Imprecise about how to evaluates "what H wants" and translate it into actions