# Problem statement
[[The Steering Problem (Christiano, 2014)]]
[[Clarifying "AI Alignment" (Christiano, 2018)]]
[[An unaligned benchmark (Christiano, 2018)]]
[[Prosaic AI alignment (Christiano, 2016)]]

# Basic intuition
[[Approval-directed agent (Christiano, 2014)]]
[[Approval-directed bootstrapping (Christiano, 2014)]]
[[Human Consulting HCH (Christiano, 2016)]]
[[Corrigibility (Christiano, 2017)]]

# The Scheme
[[Iterated Distillation and Amplification (Cotra, 2018)]]
[[Benign model-free RL (Christiano, 2017)]]
[[Factored Cognition (Stuhlm√ºller, 2018)]]
[[Supervising strong learners by amplifying weak experts (Christiano et al., 2018)]]
[[AlphaGo Zero and capability amplification (Christiano, 2017)]]

# What needs doing
[[Directions and desiderata for AI alignment (Christiano, 2017)]]
[[The reward engineering problem  (Christiano, 2016)]]
[[Capability amplification (Christiano, 2016)]]
[[Learning with catastrophes (Christiano, 2016)]]

# Possible approaches
[[Thoughts on reward engineering (Christiano, 2019)]]
[[Techniques for optimizing worst-case performance (Christiano, 2018)]]
[[Reliability amplification (Christiano, 2016)]]
[[Security amplification (Christiano, 2016)]]
[[Meta-execution (Christiano, 2018)]]

#### Tags
#ida #sequence