# Data driven emergence
[[Data Distributional Properties Drive Emergent In-Context Learning in Transformers]]
[[An Explanation of In-context Learning as Implicit Bayesian Inference]]
	-> Hidden Markov Models
	https://www.baeldung.com/cs/hidden-markov-model
	https://ai.stanford.edu/~pabbeel/depth_qual/Rabiner_Juang_hmms.pdf
	https://scholar.harvard.edu/files/adegirmenci/files/hmm_adegirmenci_2014.pdf
	https://inc.ucsd.edu/mplab/75/media//hmm.pdf
	

# Description of emergence
[[Emergent Abilities of Large Language Models]]
[[Emergent Structures and Training Dynamics in Large Language Models]]

#### 
[[Predictability and Surprise in Large Generative Models]]


# Transformers
[[Transformers from scratch]]
[[Transformer circuits]]
[[Attention is All you Need]]
	[[The Annotated Transformer]]

[[$O(n)$ Connections are Expressive Enough: Universal Approximability of Sparse Transformers]]
[[Thinking Like Transformers]]
[[Theoretical Limitations of Self-Attention in Neural Sequence Models]]


# Investigation of neural structure changes
## Norm growth
[[Effects of Parameter Norm Growth During Transformer Training - Inductive Bias from Gradient Descent]]

### Slingshot and grokking
[[The Slingshot Mechanism An Empirical Study of  Adaptive Optimizers and the Grokking Phenomenon]]


# Prompting
[[Finetuned Language Models Are Zero-Shot Learners]]
[[The Power of Scale for Parameter-Efficient Prompt Tuning]]

### Weight training VS Prompting
[[How Many Data Points is a Prompt Worth?]]

### To go further
[[Exploring Universal Intrinsic Task Subspace via Prompt Tuning]]


# Architecture search
[[Efficient Neural Architecture Search via Parameter Sharing]]
[[The Evolved Transformer]]
