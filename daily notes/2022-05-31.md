# Concept extrapolation sequence
Concept extrapolation : extending a concept to a new domain/context (for which it was not built for)

## Different perspectives on concept extrapolation
Examples and analogies

-> Working concept extrapolation could lead to AIs being able to understand our goal and needs without having to defines them explicitly

-> With concept extrapolation, AIs can build several concepts from the given data and then "ask" for which one is better for its task. 

-> CE can make a lot of alignment methods efficient. They currently are not because they need a formal def of a concept which can be broken (be put in a situation where it does not make sense or is not correlated with the intuitive definition): e.g. avoiding side effect, low impact, corrigibility. 



## Model splintering: moving from one imperfect model to another

### The big problem
Definition of model splintering : 
when the world-model changes so much that some features do not make sense anymore. The model become underdefined.

Highlight the fact that it is important to transition from imperfect model to another imperfect model. 

Problems: 
1. How to extend the prior knowledge to "out-of-distribution" environment?
2. How should the AI act "if if finds itself out-of-distribution" ?
3. How should the AI act "if if finds itself out-of-distribution, and humans don't know the correct distribution either" ?

Model splintering is best addressed directly without using "idealized perfect model". 

### Why focus on the transition
#### Humans reason like this
Human moral reasoning is built by such experiences of model splintering. This splintering led us to update/refine our moral models.

#### There are no well-defined overarching moral principles
We currently do not know how to implement idealized moral principles nor we do know how to implement an algorithm able to discover/construct it. 

#### Distinction AIs failure vs human uncertainty
||Genuine human preference not expressed in sufficient detail|Human preference fundamentally underdefined|
|-|-|-|
| Solution | More human feedback | Finding a way to resolve the ambiguity |

#### We don't need to make the problem harder
Idealized perfect model are way more complex to build than real-world moral problem. It must encapsulate solution to moral problems that do not exist in our world (e.g. willing slave race).
-> Working out non-idealized model is easier

#### We don't know how deep the rabbit hole goes
On the opposite side, the idealized perfect model build from our idealized perspective could fail completely to be accurate to the true nature of reality. 
-> If such a situation happens, model transition can cope with the splintering while idealization could get stuck.

## General alignment plus human values or via human values ?
Concept extrapolation as an alignment solution

## Value extrapolation, concept extrapolation, model splintering
Definition and disambiguation 
