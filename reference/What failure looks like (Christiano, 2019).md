# [What failure looks like](https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like)

### Part I: You get what you measure
The use of ML system narrow in easily-measured goals. 

Once the always-more-automated-by-ML society start to rely on algorithms optimizing for easily-measured proxies it will gradually shift through a situation where easily-measured proxies became the only valid metrics (and not the current real situation of the worlds) which can be trumped (e.g. reported crime rate vs criminality)

Those gradual shift will be difficult to be addressed because it of its gradual nature. 

This shift will also be accompany with a gradual transformation of the society into a more complex one, until it reach a point where human being are not able anymore to compete with the ML systems for the influence over the overall society trajectories. 

### Part II: influence-seeking behavior is scary
Influence-seeking strategies are stealth because they are indeed effective strategy (at the first glance, while training) : "performing well on the training objective is a good strategy for obtaining influence". 

Influence-seeking behavior could be the default mode, and possibly (less likely) the only one.

The problem is that it is very hard to detect because any sophisticated influence-seeking agent will do its best to game the behavioral standards applied to it.

Influence seeking leads to an even worst scenario than 1 (it is an extension of 1)
In such case, the AI does not even have the base objective as objective but only as an instrumental goal (to seek for resources/influence) 


###### Running a planning algorithm at run time:
*Modern ML instantiates _massive_ numbers of cognitive policies, and then further refines (and ultimately deploys) whatever policies perform well according to some training objective. If progress continues, eventually machine learning will probably produce systems that have a detailed understanding of the world, which are able to adapt their behavior in order to achieve specific goals.*

*Once we start searching over policies that understand the world well enough, we run into a problem: any influence-seeking policies we stumble across would also score well according to our training objective, because performing well on the training objective is a good strategy for obtaining influence.*

#FIXME 
Scenario 1 : 
- gradual outer failure by an over-optimization of easy-measured metrics
- gradual increase in complexity making humans progressively obsolete

Scenario 2 : 
- phase transition of patient power-seeking AI after a serious disruptive/disastrous event, helping AI to be set free from an easy recovery of the human society
	-> Could create/plot/force a disastrous event