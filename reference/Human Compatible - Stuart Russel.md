## 1
AI is probably on the path to superintelligence. 
Superintelligence with fixed objectives will certainly be disastrous to humanity.
This could be fixed by designing beneficial AI having *fluid objectives* based on humanity's ones. 

An AI agent of such type will often be uncertain of its objectives and could therefore exhibit desirable behaviors by relying on humans: "ask permissions, accept correction, allow themselves to be switched off".

Definite objectives must be removed from AI designs.

## 2
Definition of intelligence:
Ability to act in a way that is likely to achieve the agent's objectives given its perception of the environment.

Conscious does not matter: results will be the same with or without it. Competence does: with competence the results of the agent's actions could change drastically. 

Reward systems are great way to direct agents to act in a way which improve their fitness (e.g: fleeing from a predator, eating sweet fruits). However such system can be hacked (taking drugs...) to lead to destructive behavior (even in the natural world!). 

**AI agent cannot be trained using the *standard model* : minimizing a cost function.



## The king Midas problem
