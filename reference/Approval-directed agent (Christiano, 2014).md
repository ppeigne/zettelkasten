Source: https://www.alignmentforum.org/s/EmDuGeRw749sD3GKd/p/7Hr8t6xwuuxBTqADK
Consider: [[]]
Tags: #ida #approval-directed #act-based #preferences
______________

Agent $A$ build a model $H'$ for human $H$ preferences. Then its actions are selected by doing $\operatorname{argmax}H'(s)$. 

Understanding this paradigm implies to seriously consider the case that an agent could become fundamentally embedded with being approval-directed/act-based.

Then:
- it will not try to deceive/manipulate its overseer (because its overseer would not prefer to)
- it will try to be as interpretable as possible
- it will try to act make its overseer become as smart as possible (to make it have better preferences)
- it will ask when it is unsure (i.e. its model for the overseer is unclear about the action to take OR its model specifically understand that its is a category of actions that the overseer wants to be asked about)

Problem
-> Is it possible to make sure the agent is not internally goal-directed instead of approval-directed ?


