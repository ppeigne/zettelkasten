Previous: [[202207201648 - Data novelty and distribution as dimensions of in-context learning]]
Source: [[]]
Consider: [[]]
Tags: 
______________

What does it mean for a task to be different from the original task the model was trained on?

[[An Explanation of In-context Learning as Implicit Bayesian Inference]] task is quite precise: predicting a string of characters (a label) from an image (or to be more precise from a sequence of image-label pairs.

But what about the task claimed to be different from the task the model was trained on in traditional in-context learning setups? 

What is the pre-training task? To complete a sequence of tokens received as input by predicting the next one(s). 
What is the difference, then, between this task and a in-prompt learned task? The task will still be to complete a sequence of tokens received as input by predicting the next one(s). 



