Previous : [[]] 
Source: [[Specification gaming - the flip side of AI ingenuity (Krakovna et al., 2020)]]
Consider: [[]]
Tags: #outer_alignment
______________

Outer alignment problem corresponds to a discrepancy between the base-objective and the programmer's goal. In other words it is a problem of *framing* the programmer's goal as a functional objective for the AI system being trained. 

Reward hacking and reward tampering are problems are of such type.
In such situation, the problems is that the programmer's goal are not achieved while the reward is obtained by the system.  
